---
title: "HW3"
author: "Team 1"
date: "October 25, 2020"
output: 
  pdf_document: 
    latex_engine: xelatex
---

# 1. Data Exploration

The “Neighbourhood crime data” training data set contains 466 rows and 13 columns. The variables are thought to have a positive or negative effect on the crime rate being above median crime rate. Running a summary() function on the data set, we are able to get the mean, median, first and third quartile, and the minimum and maximum values for each variable. We included a correlation plot and pairs plot to visualize the relationship among the variables. From the correlation plot we see that dis is negatively correlated with the target (above-average median crime rate). This would suggest that lots of crime happens around these Boston employment centers. Unsurprisingly, the variables that describe a more affluent neighborhood seem to be less corrlated with crime and the variables associated with more poor neighborhoods seem more correlated with crime.
We explored the structure of the variables for both the training and evaluation data sets and finally observed how Target variable is affected by other factors. 


# 2. Data Preparation

Interestingly this data was clean and there were no NA values identified in the data set. But going through the dataset description we could identify that some values are numerical but represents certain classes and therefore should be converted into factors for the modeling process. These included chas and rad.


# 3. Build Models

We focused on building a logistic regression model since the target variable was binary variable with values limited to 0 or 1. We used the glm package available within R to perform this. We build 3 LR models using all independent variables or subsets of them and use stepwise regression. Once we trained the models, we stored the AIC, null, and residual deviance values in a table for easy representation of basic parameters of these models. When looking at the coefficients of our best model and their p-values, we keep the model because it has the best classification metrics that we care about. Most interesting is that it seems to find nox as the most significant with lowest p-value, which maybe suggests some closeness to industry.

# 4. Select Models

Out of the three models, the model that included all the parameters had lowest AIC and residual deviance value. That suggested that this model was the best model, which we verified by checking the ROC-AUC curve and the confusion matrix. We also generated the AIC for other models as well using stepAIC from the MASS package. 
Our evaluation metrics are as follows:
Accuracy: .97
Classification Error Rate: .03
Precision: .9821
Sensitivity: .9563
Specificity: .9831
F1 score: .9690
AUC: .989
True Positive: 219
False Positive: 4
True Negative: 233
False Negative: 10

Our predictions are attached in a csv called eval_preds.csv. See the bottom of the appendix for more details about the metrics of our best model.

# Appendix

# Library

```{r warning=TRUE, message=FALSE}
# load required packages
library(ggplot2)
library(dplyr)
#library(tidyr)
library(corrplot)
library(MASS)
library(caret)
library(RCurl)
library(tidyverse)
library(pROC)
library(kableExtra)
library(RCurl)
```

```{r import}
# Loading the data

git_dir <- 'https://raw.githubusercontent.com/Sizzlo/Data621/main'
train_df = read.csv(paste(git_dir, "/crime-training-data_modified.csv", sep=""))
test_df = read.csv(paste(git_dir, "/crime-evaluation-data_modified.csv", sep = ""))
head(train_df)
```

# Data Exploration & Preparation

## Summary of data
See a summary of each column in the train_df set
```{r train_dfing_data_summary}
# view a summary of all columns
summary(train_df)
```

## Structure of the data
```{r}
str(train_df)
str(test_df)
```



## NA check
```{r}
has_NA = names(which(sapply(train_df, anyNA)))
has_NA
```
There are no NAs observed


The summary() function for the training and testing data sets indicates that there are no missing values in the data. 
The response variable "target" is binary with 1 indicates crime rate is above median cirme rate and 0 indicates crime rate is not above median crime rate. 

Let's observe how the target variable is effected by other factors: </br>
1. The plot of "target" against "age" shows target equalling one (above median crime rate) increases as the proportion of owner-occupied units built prior to 1940 increaases; the boxplot further shows that a larger mean of proportions of owner-occupied units built prior to 1940 is assoicated with higher crime rate.</br>
2. Plots of crime rate against pupil-teacher ratio indicate higher crime rate "1" is associated with higher pupil-teacher ratio.

## Plotting
```{r}
par(mfrow=c(2,2))
# plot response variable "target" against predictor variable "age" 
plot(train_df$age,train_df$target)
boxplot(age ~ target, train_df )

# plot response variable "target" against predictor variable "ptratio"
plot(train_df$ptratio,train_df$target)
boxplot(ptratio ~ target, train_df)
```

## Corr analysis
```{r}
# Correlations 
cor_train <- cor(train_df,  use = "na.or.complete")
corrplot(cor_train)
```

```{r}
pairs(~ target + zn + indus
      + chas + nox + rm + age + dis + rad + tax + ptratio + lstat + medv, data = train_df)
```


## Converting to factors
```{r}
train_df$chas = as.factor(train_df$chas)
train_df$rad = as.factor(train_df$rad)

test_df$chas = as.factor(test_df$chas)
test_df$rad = as.factor(test_df$rad)
```

```{r warning=FALSE, message=FALSE}

model_metrics_df <- data.frame(Model=NA, AIC=NA, Null.Deviance=NA, Resid.Deviance=NA)


gather_metrics_func <- function(type, model_metrics_df, modelSummary) {
aic <- round(modelSummary$aic,4)
nullDeviance <- round(modelSummary$null.deviance, 4)
residDeviance <- round(modelSummary$df.residual, 4)

model_metrics_df <- rbind(model_metrics_df,c(type, aic, nullDeviance, residDeviance))
model_metrics_df <- na.omit(model_metrics_df)
return(model_metrics_df)
}
```


# Modeling


## Binary Logistic Regression
We are running Binary Logistic regression model with three 3 different set of parameters

### Modeling with Target ~ Age

```{r}
# preliminary exploration glm models
model1 <- glm(formula = target ~ age, family = binomial(), data = train_df)
summary(model1)
model_metrics_df <- gather_metrics_func('target ~ age', model_metrics_df, model1)
```


### Modelling with Target ~ ptratio

```{r}
# preliminary exploration glm models
model2 <- glm(formula = target ~ ptratio , family = binomial(), data = train_df)
summary(model2)
model_metrics_df <- gather_metrics_func('target ~ ptratio', model_metrics_df, model2)
```

### Modelling with Target ~ .(every other variable)
```{r}
all_preds = glm(target ~ ., family = binomial, data = train_df)
summary(all_preds)
model_metrics_df <- gather_metrics_func('target ~ .', model_metrics_df, all_preds)

```

## Comparing different models performance
```{r}

model_metrics_df %>% kbl() %>% kable_styling()

```


Looking at the table, we can identify on a high level that 3rd model that includes all 
the parameters is better suited. Therefore, let's come up with a confusion matrix for 3rd model that includes all the parameters.
```{r}

train_df$preds = ifelse(all_preds$fitted.values > 0.5, 1, 0)
# look at confusion matrix
cm = confusionMatrix(as_factor(train_df$preds), as_factor(train_df$target), positive = "1")
cm
```

## Using StepAIC 
Using the MASS package provided 'stepAIC' lets try to further refine the available models within it 
```{r}
step_all_preds = stepAIC(all_preds)
summary(step_all_preds)

train_df$preds = ifelse(step_all_preds$fitted.values > 0.5, 1, 0)
train_df$pred_proba = step_all_preds$fitted.values
# look at confusion matrix
cm <- confusionMatrix(as_factor(train_df$preds), as_factor(train_df$target), positive = "1")
cm
```

```{r}
hist(step_all_preds$fitted.values, main= "Histogram of Predicted Probabilities", xlab="Predicted Probabilities")
```

# Plotting ROC 
```{r}
proc = roc(train_df$target, train_df$pred_proba)
plot(proc, asp=NA, legacy.axes=TRUE, print.auc=TRUE, xlab="Specificity")
```

```{r}
recall = .9563
precision = .9821
f1 = 2*precision*recall/(precision+recall)
f1
```

# Conclusion
Using the above defined steps where using stepAIC and confusionMatrix we can derive at the model that has below specifications </br>
Sensitivity : 0.9563 </br>
Specificity : 0.9831 </br>
Accuracy: .97 </br>
Precision: 0.9821 </br>
AUC: .989

# Predictions on evaluation set
```{r}
model = step_all_preds
test_preds = round(predict(model, newdata=test_df, type='response')) #*162
test_df$PRED_TARGET = test_preds
write.csv(test_df, 'eval_with_preds.csv')
```